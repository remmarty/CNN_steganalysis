{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pandas\n",
    "import seaborn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torchmetrics import *\n",
    "import separableconv.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.checkpoint as cp\n",
    "import torch.utils.data as ud\n",
    "from torch.nn.utils import clip_grad as cg\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, RandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from neptune.new.types import File\n",
    "\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if its possible to use GPU/CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.9,max_split_size_mb:128\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "print(torch.version.cuda)\n",
    "print(torch.version)\n",
    "print(torchvision.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"train_batch_size\": 16,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 16,\n",
    "    \"batch_norm_momentum\": 0.2,\n",
    "    \"batch_norm_epsilon\": 0.001,\n",
    "    \"lr\": 0.001,\n",
    "    \"Adam_beta1\": 0.9,\n",
    "    \"Adam_beta2\": 0.999,\n",
    "    \"Adam_epsilon\": 1e-8,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"max_epochs\": 100,\n",
    "    #\"gamma\": 0.5,\n",
    "    #\"schedule\": [30, 50, 75, 90]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'E:/WOW_04BPP/train'\n",
    "val_dir = 'E:/WOW_04BPP/val'\n",
    "test_dir = 'E:/WOW_04BPP/test'\n",
    "\n",
    "class Data(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.num_workers = 0\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train = ImageFolder(root=train_dir, transform=self.transform)\n",
    "        self.val = ImageFolder(root=val_dir, transform=self.transform)\n",
    "        self.test = ImageFolder(root=test_dir, transform=self.transform)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            train_indices = list(range(len(self.train)))\n",
    "            train_sampler = RandomSampler(train_indices, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(42))\n",
    "            self.train_set = DataLoader(self.train, batch_size=params[\"train_batch_size\"], sampler=train_sampler,\n",
    "                                         num_workers=self.num_workers, pin_memory=True, drop_last=True) \n",
    "            val_indices = list(range(len(self.val)))\n",
    "            val_sampler = RandomSampler(val_indices, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(42))\n",
    "            self.val_set = DataLoader(self.val, batch_size=params[\"val_batch_size\"], sampler=val_sampler,\n",
    "                                    num_workers=self.num_workers, pin_memory=True, drop_last=True)\n",
    "            \n",
    "        if stage == \"test\": \n",
    "            test_indices = list(range(len(self.test)))\n",
    "            test_sampler = RandomSampler(test_indices, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(42))\n",
    "            self.test_set = DataLoader(self.test, batch_size=params[\"test_batch_size\"], sampler=test_sampler, \n",
    "                                    num_workers=self.num_workers, pin_memory=True, drop_last=True)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return self.train_set\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_set\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_set\n",
    "    \n",
    "data_module = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Abs, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.abs(x)\n",
    "        return output\n",
    "    \n",
    "class Trunc(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(Trunc, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.clamp(x, min = -self.threshold, max = self.threshold)\n",
    "        return output\n",
    "    \n",
    "class ScaleLayer(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ScaleLayer, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.gamma.view(1, -1, 1, 1) + self.beta.view(1, -1, 1, 1)\n",
    "\n",
    "def Tanh3(x):\n",
    "    tanh3 = 3 * torch.tanh(x)\n",
    "    return tanh3\n",
    "\n",
    "def plot_roc(y, preds, pred_scores, is_test):\n",
    "    fpr_0, tpr_0, _ = roc_curve(1 - y, 1 - pred_scores)\n",
    "    roc_auc_0 = auc(fpr_0, tpr_0)\n",
    "\n",
    "    fpr_1, tpr_1, _ = roc_curve(y, preds[:, 1])\n",
    "    roc_auc_1 = auc(fpr_1, tpr_1)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(fpr_0, tpr_0, label='Class 0: Cover (AUC = {:.2f})'.format(roc_auc_0))\n",
    "    plt.plot(fpr_1, tpr_1, label='Class 1: Stego (AUC = {:.2f})'.format(roc_auc_1))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Random (AUC = 0.5)')\n",
    "    plt.title(\"Receiver Operating Characteristics\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    if is_test:\n",
    "        neptune_logger.experiment[\"test_ROC\"].append(File.as_image(fig))\n",
    "\n",
    "def plot_confusion_matrix(confusion_mat, is_test):\n",
    "    classes = ['Cover', 'Stego']\n",
    "\n",
    "    confusion_mat = confusion_mat.astype('float') / confusion_mat.sum(axis=1)[:, np.newaxis]\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    for i in range(confusion_mat.shape[0]):\n",
    "        for j in range(confusion_mat.shape[1]):\n",
    "            plt.text(j, i, format(confusion_mat[i, j], '.2f'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if confusion_mat[i, j] > 0.5 else \"black\")\n",
    "\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    if is_test:\n",
    "        neptune_logger.experiment[\"test_CM\"].append(File.as_image(fig))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srm_weights = np.load('SRM_filters.npy')*(1/12)\n",
    "biasSRM = np.ones(30)\n",
    "\n",
    "srm_weights_tensor = torch.from_numpy(srm_weights).permute(3,2,0,1).cuda()\n",
    "biasSRM_tensor = torch.from_numpy(biasSRM).cuda().float()\n",
    "\n",
    "### Define CNN architecture ###\n",
    "class GBRAS(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.val_ytrue = []\n",
    "        self.val_ypred = []\n",
    "        self.test_ytrue = []\n",
    "        self.test_ypred = []\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "        self.srm_weights = torch.nn.Parameter(srm_weights_tensor, requires_grad=False)\n",
    "        self.biasSRM = torch.nn.Parameter(biasSRM_tensor, requires_grad=True)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=30, momentum=params[\"batch_norm_momentum\"],eps=params[\"batch_norm_epsilon\"],\n",
    "                                   affine=True)\n",
    "        \n",
    "        self.depthwise_separable_conv1 = separableconv.nn.SeparableConv2d(in_channels=30, out_channels=30, kernel_size=3, stride=1, padding=\"same\",\n",
    "                                                            depth_multiplier=3, activation_dw=nn.ELU, activation_pw=nn.ELU)\n",
    "\n",
    "        self.depth_bn1 = nn.BatchNorm2d(num_features=30, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "        \n",
    "        self.depthwise_separable_conv2 = separableconv.nn.SeparableConv2d(in_channels=30, out_channels=30, kernel_size=3, stride=1, padding=\"same\",\n",
    "                                                            depth_multiplier=3, activation_dw=nn.ELU, activation_pw=nn.ELU)\n",
    "       \n",
    "        self.depth_bn2 = nn.BatchNorm2d(num_features=30, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"],\n",
    "                                   affine=True) \n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=30, out_channels=30, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=30, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=30, out_channels=30, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=30, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=30, out_channels=60, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=60, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "\n",
    "        self.depthwise_separable_conv3 = separableconv.nn.SeparableConv2d(in_channels=60, out_channels=60, kernel_size=3, stride=1, padding=\"same\",\n",
    "                                                           depth_multiplier=3, activation_dw=nn.ELU, activation_pw=nn.ELU)\n",
    "        \n",
    "        self.depth_bn3 = nn.BatchNorm2d(num_features=60, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "\n",
    "        self.depthwise_separable_conv4 = separableconv.nn.SeparableConv2d(in_channels=60, out_channels=60, kernel_size=3, stride=1, padding=\"same\",\n",
    "                                                           depth_multiplier=3, activation_dw=nn.ELU, activation_pw=nn.ELU)\n",
    "      \n",
    "        self.depth_bn4 = nn.BatchNorm2d(num_features=60, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=60, out_channels=60, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=60, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "          \n",
    "        self.conv6 = nn.Conv2d(in_channels=60, out_channels=60, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=60, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(in_channels=60, out_channels=60, kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.bn7 = nn.BatchNorm2d(num_features=60, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(in_channels=60, out_channels=30, kernel_size=1, stride=1, padding=\"same\")\n",
    "        self.bn8 = nn.BatchNorm2d(num_features=30, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "        \n",
    "        self.conv9 = nn.Conv2d(in_channels=30, out_channels=2, kernel_size=1, stride=1, padding=\"same\")\n",
    "        self.bn9 = nn.BatchNorm2d(num_features=2, momentum=params[\"batch_norm_momentum\"], eps=params[\"batch_norm_epsilon\"], affine=True)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if type(m) == (nn.Linear or nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "    def forward(self, layers):\n",
    "        layers = F.conv2d(layers, self.srm_weights, self.biasSRM, stride=1, padding=\"same\")\n",
    "        layers = Tanh3(layers)  \n",
    "        layers1 = self.bn1(layers)\n",
    "        \n",
    "        layers = self.depthwise_separable_conv1(layers1)\n",
    "        layers = self.depth_bn1(layers)\n",
    "        \n",
    "        layers = self.depthwise_separable_conv2(layers)\n",
    "        layers2 = self.depth_bn2(layers)\n",
    "    \n",
    "        skip1 = torch.add(layers1, layers2)\n",
    "        \n",
    "        layers = self.conv2(skip1)\n",
    "        layers = self.elu(layers)\n",
    "        layers = self.bn2(layers)\n",
    "       \n",
    "        layers = self.conv3(layers)\n",
    "        layers = self.elu(layers)\n",
    "        layers = self.bn3(layers)\n",
    "\n",
    "        layers = nn.AvgPool2d(kernel_size=5, stride=2, padding=2)(layers)\n",
    "        layers = self.conv4(layers)\n",
    "        layers = self.elu(layers)\n",
    "        layers3 = self.bn4(layers)\n",
    "\n",
    "        layers = self.depthwise_separable_conv3(layers3)\n",
    "        layers = self.depth_bn3(layers)\n",
    "\n",
    "        layers = self.depthwise_separable_conv4(layers)\n",
    "        layers4 = self.depth_bn4(layers)\n",
    "\n",
    "        skip2 = torch.add(layers3, layers4)\n",
    "        \n",
    "        layers = self.conv5(skip2)\n",
    "        layers = self.elu(layers)\n",
    "        layers = self.bn5(layers)\n",
    "\n",
    "        layers = nn.AvgPool2d(kernel_size=5, stride=2, padding=2)(layers)\n",
    "        layers = self.conv6(layers)\n",
    "        layers = self.elu(layers)\n",
    "        layers = self.bn6(layers)\n",
    "      \n",
    "        layers = nn.AvgPool2d(kernel_size=5, stride=2, padding=2)(layers)\n",
    "        layers = self.conv7(layers)\n",
    "        layers = self.elu(layers)\n",
    "        layers = self.bn7(layers)\n",
    "\n",
    "        layers = nn.AvgPool2d(kernel_size=5, stride=2, padding=2)(layers)\n",
    "        layers = self.conv8(layers)\n",
    "        layers = self.elu(layers)\n",
    "        layers = self.bn8(layers)\n",
    "\n",
    "        layers = self.conv9(layers)\n",
    "        layers = self.elu(layers)\n",
    "        layers = self.bn9(layers)\n",
    "\n",
    "        layers = F.adaptive_avg_pool2d(layers, (1, 1))\n",
    "        layers = layers.view(-1, 2)\n",
    "        layers = F.softmax(layers, dim=1)\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=params[\"lr\"], betas=(params[\"Adam_beta1\"], params[\"Adam_beta2\"]),\n",
    "                eps=params[\"Adam_epsilon\"], weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "        #scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=params[\"schedule\"], gamma=params[\"gamma\"])\n",
    "\n",
    "        return [optimizer]#, [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "       \n",
    "        target = F.one_hot(y, num_classes=2).float()\n",
    "        preds = pred.float()\n",
    "\n",
    "        loss = F.binary_cross_entropy(preds, target)\n",
    "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        y_true = y\n",
    "        y_pred = pred.argmax(dim=1)\n",
    "\n",
    "        acc = accuracy_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        self.log(\"train/acc\", acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "\n",
    "        target = F.one_hot(y, num_classes=2).float()\n",
    "        preds = pred.float()\n",
    "        \n",
    "        loss = F.binary_cross_entropy(preds, target)\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        y_true = y\n",
    "        y_pred = pred.argmax(dim=1)\n",
    "\n",
    "        acc = accuracy_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        self.log(\"val/acc\", acc, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.val_ytrue.append(y.detach())\n",
    "        self.val_ypred.append(pred.detach())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        y = torch.stack(self.val_ytrue).cpu().numpy()\n",
    "        preds = torch.stack(self.val_ypred).cpu().numpy()\n",
    "        \n",
    "        y = torch.tensor(y).reshape(-1)\n",
    "        preds = torch.tensor(preds).reshape(-1, 2)\n",
    "\n",
    "        #pred_scores = preds[:, 1]\n",
    "\n",
    "        #plot_roc(y, preds, pred_scores, False)\n",
    "            \n",
    "        #cm_pred = (pred_scores.numpy() >= 0.5).astype(int)\n",
    "        #cm = confusion_matrix(y, cm_pred)\n",
    "        #plot_confusion_matrix(cm, False)\n",
    "            \n",
    "        self.val_ytrue.clear()\n",
    "        self.val_ypred.clear()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "\n",
    "        target = F.one_hot(y, num_classes=2).float()\n",
    "        preds = pred.float()\n",
    "        \n",
    "        loss = F.binary_cross_entropy(preds, target)\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.test_ytrue.append(y.detach())\n",
    "        self.test_ypred.append(pred.detach())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        y = torch.stack(self.test_ytrue).cpu().numpy()\n",
    "        preds = torch.stack(self.test_ypred).cpu().numpy()\n",
    "        \n",
    "        y = torch.tensor(y).reshape(-1)\n",
    "        preds = torch.tensor(preds).reshape(-1, 2)\n",
    "\n",
    "        acc_metric = torchmetrics.Accuracy(task='multiclass', num_classes=2)\n",
    "        acc = acc_metric(preds, y)\n",
    "        self.log(\"test/acc\", acc)\n",
    "\n",
    "        prec_metric = torchmetrics.Precision(task='multiclass', num_classes=2)\n",
    "        prec = prec_metric(preds, y)\n",
    "        self.log(\"test/prec\", prec)\n",
    "\n",
    "        recall_metric = torchmetrics.Recall(task='multiclass', num_classes=2)\n",
    "        recall = recall_metric(preds, y)\n",
    "        self.log(\"test/recall\", recall)\n",
    "        \n",
    "        f1_metric = torchmetrics.F1Score(task='multiclass', num_classes=2)\n",
    "        f1 = f1_metric(preds, y)\n",
    "        self.log(\"test/f1\", f1)\n",
    "\n",
    "        pred_scores = preds[:, 1]\n",
    "    \n",
    "        plot_roc(y, preds, pred_scores, True)\n",
    "            \n",
    "        cm_pred = (pred_scores.numpy() >= 0.5).astype(int)\n",
    "        cm = confusion_matrix(y, cm_pred)\n",
    "        plot_confusion_matrix(cm, True)\n",
    "            \n",
    "        self.test_ytrue.clear()\n",
    "        self.test_ypred.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NeptunLogger\n",
    "neptune_logger = NeptuneLogger(\n",
    "    api_key=\"\", # you must enter your own account api key generated by Neptune.ai\n",
    "    project=\"\", # same with the project name etc.\n",
    "    prefix=\"experiment\",\n",
    "    tags=[\"BossBase_8/1/1, srm filters\"],\n",
    "    log_model_checkpoints=True, \n",
    "    capture_hardware_metrics=False,\n",
    "    capture_stdout=False,\n",
    "    source_files=\"GBRAS_16.ipynb\"\n",
    ")\n",
    "\n",
    "run_id = neptune_logger.run[\"sys/id\"].fetch()\n",
    "folder_name = run_id\n",
    "log_path = \"E:/GBRAS_WOW_CHECKPOINTS_google\"\n",
    "\n",
    "path = os.path.join(log_path, folder_name)\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Create learning rate logger\n",
    "lr_logger = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "# Create model checkpointing object\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=path,\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/acc\",\n",
    "    save_weights_only=False,\n",
    "    save_top_k=100,\n",
    "    save_last=True,\n",
    "    every_n_epochs=1,\n",
    ")\n",
    "\n",
    "# Initialize a trainer and pass neptune_logger\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=path,\n",
    "    logger=neptune_logger,\n",
    "    callbacks=[lr_logger, model_checkpoint],\n",
    "    log_every_n_steps=150,\n",
    "    accelerator=\"cuda\",\n",
    "    devices=1,\n",
    "    max_epochs=params[\"max_epochs\"],\n",
    "    val_check_interval=1.0,\n",
    "    enable_progress_bar=True,\n",
    "    num_sanity_val_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GBRAS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log model summary\n",
    "neptune_logger.log_model_summary(model=model, max_depth=-1)\n",
    "\n",
    "# Log hyperparameters\n",
    "neptune_logger.log_hyperparams(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = model_checkpoint.best_model_path\n",
    "print(best_model_path)\n",
    "best_model = GBRAS.load_from_checkpoint(best_model_path)\n",
    "trainer.test(best_model, datamodule=data_module)\n",
    "\n",
    "neptune_logger.experiment.stop()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stego",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
